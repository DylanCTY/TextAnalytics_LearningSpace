{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsP7pLQj9TL4jJvVtEpwwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DylanCTY/TextAnalytics_LearningSpace/blob/main/IB9CW0_5504008_pending.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "myrb0_Mc2pKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPYwcYhz5Bvb",
        "outputId": "b43fc18a-759e-43d8-fdaa-4cc8eb3cf0f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import uuid\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def document_chunker(file_path,\n",
        "                     model_name,\n",
        "                     paragraph_separator='\\n\\n',\n",
        "                     chunk_size=256,\n",
        "                     separator=' ',\n",
        "                     secondary_chunking_regex=r'\\S+?[\\.,;!?]',\n",
        "                     chunk_overlap=0):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer for the specified model\n",
        "    documents = {}  # Initialize dictionary to store results\n",
        "\n",
        "    base = os.path.basename(file_path)\n",
        "    sku = os.path.splitext(base)[0]\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Generate a unique identifier for the document\n",
        "        doc_id = str(uuid.uuid4())\n",
        "\n",
        "        # Process the file using the existing chunking logic\n",
        "        paragraphs = re.split(paragraph_separator, text)\n",
        "        all_chunks = {}\n",
        "        for paragraph in paragraphs:\n",
        "            words = paragraph.split(separator)\n",
        "            current_chunk = \"\"\n",
        "            chunks = []\n",
        "\n",
        "            for word in words:\n",
        "                new_chunk = current_chunk + (separator if current_chunk else '') + word\n",
        "                if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
        "                    current_chunk = new_chunk\n",
        "                else:\n",
        "                    if current_chunk:\n",
        "                        chunks.append(current_chunk)\n",
        "                    current_chunk = word\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "\n",
        "            refined_chunks = []\n",
        "            for chunk in chunks:\n",
        "                if len(tokenizer.tokenize(chunk)) > chunk_size:\n",
        "                    sub_chunks = re.split(secondary_chunking_regex, chunk)\n",
        "                    sub_chunk_accum = \"\"\n",
        "                    for sub_chunk in sub_chunks:\n",
        "                        if sub_chunk_accum and len(tokenizer.tokenize(sub_chunk_accum + sub_chunk + ' ')) > chunk_size:\n",
        "                            refined_chunks.append(sub_chunk_accum.strip())\n",
        "                            sub_chunk_accum = sub_chunk\n",
        "                        else:\n",
        "                            sub_chunk_accum += (sub_chunk + ' ')\n",
        "                    if sub_chunk_accum:\n",
        "                        refined_chunks.append(sub_chunk_accum.strip())\n",
        "                else:\n",
        "                    refined_chunks.append(chunk)\n",
        "\n",
        "            final_chunks = []\n",
        "            if chunk_overlap > 0 and len(refined_chunks) > 1:\n",
        "                for i in range(len(refined_chunks) - 1):\n",
        "                    final_chunks.append(refined_chunks[i])\n",
        "                    overlap_start = max(0, len(refined_chunks[i]) - chunk_overlap)\n",
        "                    overlap_end = min(chunk_overlap, len(refined_chunks[i+1]))\n",
        "                    overlap_chunk = refined_chunks[i][overlap_start:] + ' ' + refined_chunks[i+1][:overlap_end]\n",
        "                    final_chunks.append(overlap_chunk)\n",
        "                final_chunks.append(refined_chunks[-1])\n",
        "            else:\n",
        "                final_chunks = refined_chunks\n",
        "\n",
        "            # Assign a UUID for each chunk and structure it with text and metadata\n",
        "            for chunk in final_chunks:\n",
        "                chunk_id = str(uuid.uuid4())\n",
        "                all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"file_name\":sku}}  # Initialize metadata as dict\n",
        "\n",
        "        # Map the document UUID to its chunk dictionary\n",
        "        documents[doc_id] = all_chunks\n",
        "\n",
        "    return documents\n",
        "\n",
        "# Assuming the file is uploaded in Colab and the path is specified as '/content/GOTbook.txt'\n",
        "file_path = '/content/1_A_Game_of_Thrones.txt'\n",
        "model_name = 'BAAI/bge-small-en-v1.5'  # Replace with your specific model if different\n",
        "\n",
        "# Call the function\n",
        "docs = document_chunker(file_path, model_name, chunk_size=256)\n",
        "\n",
        "# Print the length and the first chunk of the document\n",
        "keys = list(docs.keys())\n",
        "print(len(docs))\n",
        "print(docs[keys[0]])\n"
      ],
      "metadata": {
        "id": "2lBxLIh82WJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "508d7936-a349-4692-b43b-a213c22727af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-417abf45aeac>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = 'BAAI/bge-small-en-v1.5'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.save_pretrained(\"model/tokenizer\")\n",
        "model.save_pretrained(\"model/embedding\")"
      ],
      "metadata": {
        "id": "SZoK-5o5JWhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(text):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"model/tokenizer\")\n",
        "    model = AutoModel.from_pretrained(\"model/embedding\")\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Generate the embeddings\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "    return embeddings.tolist()"
      ],
      "metadata": {
        "id": "TFGlWd3FJjCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(doc_store):\n",
        "    vector_store = {}\n",
        "    for doc_id, chunks in doc_store.items():\n",
        "        doc_vectors = {}\n",
        "        for chunk_id, chunk_dict in chunks.items():\n",
        "            # Generate an embedding for each chunk of text\n",
        "            doc_vectors[chunk_id] = compute_embeddings(chunk_dict.get(\"text\"))\n",
        "        # Store the document's chunk embeddings mapped by their chunk UUIDs\n",
        "        vector_store[doc_id] = doc_vectors\n",
        "    return vector_store\n"
      ],
      "metadata": {
        "id": "x8PmD2N7JkuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_store = create_vector_store(docs)"
      ],
      "metadata": {
        "id": "jinwHidYQQE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_matches(vector_store, query_str, top_k):\n",
        "    \"\"\"\n",
        "    This function takes in a vector store dictionary, a query string, and an int 'top_k'.\n",
        "    It computes embeddings for the query string and then calculates the cosine similarity against every chunk embedding in the dictionary.\n",
        "    The top_k matches are returned based on the highest similarity scores.\n",
        "    \"\"\"\n",
        "    # Get the embedding for the query string\n",
        "    query_str_embedding = np.array(compute_embeddings(query_str))\n",
        "    scores = {}\n",
        "\n",
        "    # Calculate the cosine similarity between the query embedding and each chunk's embedding\n",
        "    for doc_id, chunks in vector_store.items():\n",
        "        for chunk_id, chunk_embedding in chunks.items():\n",
        "            chunk_embedding_array = np.array(chunk_embedding)\n",
        "            # Normalize embeddings to unit vectors for cosine similarity calculation\n",
        "            norm_query = np.linalg.norm(query_str_embedding)\n",
        "            norm_chunk = np.linalg.norm(chunk_embedding_array)\n",
        "            if norm_query == 0 or norm_chunk == 0:\n",
        "                # Avoid division by zero\n",
        "                score = 0\n",
        "            else:\n",
        "                score = np.dot(chunk_embedding_array, query_str_embedding) / (norm_query * norm_chunk)\n",
        "\n",
        "            # Store the score along with a reference to both the document and the chunk\n",
        "            scores[(doc_id, chunk_id)] = score\n",
        "\n",
        "    # Sort scores and return the top_k results\n",
        "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
        "    top_results = [(doc_id, chunk_id, score) for ((doc_id, chunk_id), score) in sorted_scores]\n",
        "\n",
        "    return top_results"
      ],
      "metadata": {
        "id": "xheg67JuIgTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Example usage\n",
        "matches = compute_matches(vector_store=vec_store, query_str=\"Who is Jon Snow's mother?\", top_k=3)\n",
        "\n",
        "# Print the top matches\n",
        "for match in matches:\n",
        "    print(f\"Document ID: {match[0]}, Chunk ID: {match[1]}, Similarity Score: {match[2]}\")"
      ],
      "metadata": {
        "id": "efl6p47BJJ5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plug the top match document ID keys into doc_store to access the retrieved content\n",
        "docs['3dbd9ba7-84d3-4124-9707-2e72438a50b6']['bb281824-01f8-47d2-979f-aad1f917efc3']\n"
      ],
      "metadata": {
        "id": "yY6QB_NmJJuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure you have the necessary packages installed\n",
        "!pip install transformers sentence-transformers\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "# Use GPT-2 for streaming responses\n",
        "def stream_and_buffer(base_prompt, llm, max_tokens=800, stop=None, echo=True, stream=True):\n",
        "    # Formatting the base prompt\n",
        "    formatted_prompt = f\"Q: {base_prompt} A: \"\n",
        "\n",
        "    inputs = llm.tokenizer.encode(formatted_prompt, return_tensors='pt')\n",
        "    outputs = llm.model.generate(inputs, max_length=max_tokens, pad_token_id=llm.tokenizer.eos_token_id, stop=stop)\n",
        "\n",
        "    response = llm.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    buffer = response[len(formatted_prompt):]  # Remove the prompt from the response\n",
        "\n",
        "    sys.stdout.write(buffer)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def construct_prompt(system_prompt, retrieved_docs, user_query):\n",
        "    prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "Here is the retrieved context:\n",
        "{retrieved_docs}\n",
        "\n",
        "Here is the user's query:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Usage example\n",
        "system_prompt = \"\"\"\n",
        "You are a knowledgeable historian of Westeros. You will be provided with some context from the texts of Game of Thrones, as well as the user's query.\n",
        "\n",
        "Your job is to understand the request, and answer based on the provided context.\n",
        "\"\"\"\n",
        "\n",
        "retrieved_docs = \"\"\"\n",
        "Jon Snow is the son of Lyanna Stark and Rhaegar Targaryen. He was raised as the illegitimate son of Eddard Stark in Winterfell.\n",
        "\"\"\"\n",
        "\n",
        "prompt = construct_prompt(system_prompt=system_prompt,\n",
        "                          retrieved_docs=retrieved_docs,\n",
        "                          user_query=\"Who are the parents of John Snow?\")\n",
        "\n",
        "# Initialize the GPT-2 model and tokenizer\n",
        "class LLMWrapper:\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "llm = LLMWrapper(model_name=\"gpt2\")\n",
        "\n",
        "# Stream and buffer the response\n",
        "stream_and_buffer(prompt, llm)"
      ],
      "metadata": {
        "id": "otWDrk6CoBQ2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama v0.1.30\n",
        "!curl https://ollama.ai/install.sh | sed 's#https://ollama.ai/download#https://github.com/jmorganca/ollama/releases/download/v0.1.30#' | sh\n",
        "# Setup the model as a global variable\n",
        "OLLAMA_MODEL='phi:latest'\n",
        "\n",
        "# Add the model to the environment of the operating system\n",
        "import os\n",
        "os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL\n",
        "!echo $OLLAMA_MODEL # print the global variable to check it saved\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama on the server (\"serve\")\n",
        "command = \"nohup ollama serve&\" # \"nohup\" and \"&\" means run in the background\n",
        "\n",
        "# Use subprocess.Popen to run the command\n",
        "process = subprocess.Popen(command,\n",
        "                            shell=True,\n",
        "                            stdout=subprocess.PIPE,\n",
        "                            stderr=subprocess.PIPE)\n",
        "\n",
        "print(\"Process ID:\", process.pid) # print the process ID\n",
        "time.sleep(5)  # Makes Python wait for 5 seconds\n",
        "\n",
        "!ollama -v # print the Ollama version number as a check\n"
      ],
      "metadata": {
        "id": "v2NsaEkjunQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-ollama\n",
        "!pip install llama-index-vector-stores-chroma\n",
        "!pip install llama-index ipywidgets\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install chromadb\n"
      ],
      "metadata": {
        "id": "uDE896Qquqr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "import sys\n",
        "\n",
        "# Initialize the Ollama LLM\n",
        "llm = Ollama(model=OLLAMA_MODEL, request_timeout=240.0)\n",
        "\n",
        "def stream_and_buffer(base_prompt, llm, max_tokens=800, stop=None, echo=True, stream=True):\n",
        "    # Formatting the base prompt\n",
        "    formatted_prompt = f\"Q: {base_prompt} A: \"\n",
        "\n",
        "    # Assuming `llm` has a method to run the model and get responses\n",
        "    response = llm.run(formatted_prompt, max_tokens=max_tokens)\n",
        "\n",
        "    buffer = response[len(formatted_prompt):]  # Remove the prompt from the response\n",
        "\n",
        "    sys.stdout.write(buffer)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def construct_prompt(system_prompt, retrieved_docs, user_query):\n",
        "    prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "Here is the retrieved context:\n",
        "{retrieved_docs}\n",
        "\n",
        "Here is the user's query:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Usage example\n",
        "system_prompt = \"\"\"\n",
        "You are a knowledgeable historian of Westeros. You will be provided with some context from the texts of Game of Thrones, as well as the user's query.\n",
        "\n",
        "Your job is to understand the request, and answer based on the provided context.\n",
        "\"\"\"\n",
        "\n",
        "retrieved_docs = \"\"\"\n",
        "Jon Snow is the son of Lyanna Stark and Rhaegar Targaryen. He was raised as the illegitimate son of Eddard Stark in Winterfell.\n",
        "\"\"\"\n",
        "\n",
        "prompt = construct_prompt(system_prompt=system_prompt,\n",
        "                          retrieved_docs=retrieved_docs,\n",
        "                          user_query=\"Who are the parents of Jon Snow?\")\n",
        "\n",
        "# Stream and buffer the response\n",
        "stream_and_buffer(prompt, llm)\n"
      ],
      "metadata": {
        "id": "QJ8QG05qusGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
